# Нейронный Детектор машинно-сгенерированного текста.

## Постановка задачи

Для прикладных целей бывает полезно понимать, является ли заданный текст AI-generated. Например это надо для образовательных целей - антиплагиат система для проверки сам ли делал задание студент.

### Формат входных и выходных данных

Входные данные: текст (строка)

Выходные данные: вероятность того что текст сгенерирован AI.

### Метрики

accuracy, f1, roc_auc - классические метрики для бинарной классификации. Датасет будет сбалансирован, поэтому с этим все окей.

### Валидация и тест

Разделение выборки - 50%(label 0) 50%(label 1). Причем все данные для простоты будут из одного domain. Среди текстов с (label 1) мы хотим равномерно распределить по генерирующим моделям(возьмем их несколько). Train / Test 80 : 20, причем там все также равномерно.

### Датасет

https://github.com/liamdugan/raid

6M текстов из самого большого бенчмарка для проверки детекторов. Он содержит множество текстов на разные темы (Reddit, Wiki, arXiv...), часть из которых либо написана человеком, либо сгенерированна одной из нейронок (Claude, Gemini, GPT4...). Тексты содержат различные атаки на текст (перифраз, моноглифы...) для повышения робастности детекторов. Мы берем подвыборку, примерно 30k текстов.

Код для создания train.csv и test.csv (создания!! его не нужно запускать, так как он жрет много памяти. Данные получаем через `dvc pull` из MinIO), вот [тут](data/download_data.py).

## Моделирование

### Бейзлайн

Мы хотим превзойти dummy-модель `f(x) = random.randint(0,1)`. Таким образом нужно, чтобы наша модель выбивала `accuracy > 0.5`

### Основная модель

Используем дообученный https://huggingface.co/prajjwal1/bert-tiny размороженный на всех слоях.

```
learning_rate: 3e-5
epochs: 3
weight_decay: 0.01
```

### Внедрение

Можно использовать модель как детектор. Для есть Triton Inference Server.

## Технологии:

- poetry
- pre-commit
- pytorch Lightning
- dvc через MinIO
- hydra
- wandb
- onnx
- triton inference server

## Развертывание проекта.

### Setup

Клонируем репозиторий.

```
git clone https://github.com/MonosUna/AIDetector.git
cd AIDetector
```

Устанавливаем зависимости.

```
pip install poetry==2.2.1
poetry install
```

Подтягиваем хуки, и проверяем все ли ок.

```
poetry run pre-commit install
poetry run pre-commit run -a
```

### Train

Активируем виртуальное окружение.(poetry выдаст команду для активации env).

```
poetry env activate
```

Скачиваем данные из MiniO

```
dvc pull
```

Готовим данные к обучению.

```
python3 data/scripts/prepare.py
```

Для логирования используется wandb, поэтому если в вашей системы нет их API ключа, то нужно его ввести. Для этого вводим, переходим в личный кабинет, копируем API ключ и вставляем в терминал. ВНИМАНИЕ: обучение не будет работать без wandb, а он работает только с VPN.

```
poetry run wandb login --relogin
```

Можно запускать обучение. по дефолту [модель](model/detector_local_training) уже скачана с помощью dvc. Новая обученная модель будем сохранена там же.

```
poetry run python ai_detector/train/train.py
```

Чтобы посмотреть метрики обученной модели на тестовом датасете:

```
poetry run python ai_detector/test/test.py
```

## Production preparation

Конвертируем модель в onnx формат для triton сервера.

```
poetry run python ai_detector/modules/to_onnx.py
```

## Infer

Запускаем triton сервер и веб-приложение(возможно придется использовать несколько терминалов).

```
cd triton
docker-compose up
poetry run python ai_detector/web/app.py
```

Сервер доступен на `http://localhost:8080`.

Остановливаем через `Ctrl+C`.
